"""
Training code for GONet DCGAN
@author Nick Goodson
"""
import sys
import os
import argparse
import time

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.utils.data import RandomSampler
import torchvision.transforms as T
import torchvision.utils as vutils
from torchvision.utils import save_image

import numpy as np
import matplotlib.pyplot as plt

from DataSetDCGAN import DataSetDCGAN
from GONet_torch import Generator, Discriminator, weights_init
from utils import Normalize, display_num_images, save_model_params, load_model_params

# ********* Change these paths for your computer **********************
DATA_PATH = "/home/lrf/GO_Data_GAN" # path on Google Cloud VM
SAVE_PATH = "/home/lrf/DCGAN_Model" # path on Google Cloud VM
IMAGE_SAVE_PATH = "/home/lrf/GAN_Img" # path on Google Cloud VM
USE_GPU = True # true for Google Cloud VM
DTYPE = torch.float32
WORKERS = 8  # number of threads for Dataloaders (0 = singlethreaded)
IMAGE_SIZE = 128
# RANDOM_SEED = 291
PRINT_EVERY = 20  # num iterations between printing learning stats
SAVE_EVERY = 1  # num iterations to save weights after (Assign to 'None' to turn off)
SAVE_TIME_INTERVAL = 60 * 20  # save model every 20 minutes
TEST_GEN_EVERY = 30  # how often (in iterations) to check images gen creates from the fixed noise

if USE_GPU and torch.cuda.is_available():
	device = torch.device('cuda')
else:
	device = torch.device('cpu')
print(f"using device: {device}")

# torch.manual_seed(RANDOM_SEED) not needed for Google Cloud VM


# Hyper parameters
########################################
beta1 = 0.5  # For ADAM optimizer
batch_size = 64
num_epochs = 1
lr_gen = 0.0004
lr_dis = 0.0001
from GONet_torch import nz  # size of latent vector z
label_smoothing_start = 15  # what iteration to start label smoothing
#######################################


def train_dcgan(gen, dis, optimizer_g, optimizer_d, loss_fn, loader_train, loader_val):
	"""
	Training the DCGAN on GONet data-set using Pytorch.
	Training is done on all positive examples from the data-set
	The generator Gen(z) images are labelled with 0.0 (or 0.1)
	The true images are labelled with 1.0 (or 0.9)

	Inputs:
	- gen: The generator network
	- dis: The discriminator network
	- optimizer_g: Optimizer object for gen
	- optimizer_d: Optimizer object for dis
	- loss_fn: The loss function to optimize
	- loader_train: Dataloader object for training data
	- loader_val: Dataloader object for validation data

	Returns:
	- gen_test_imgs: a list of grids of images generated by gen during training
	- gen_loss_hist: gen loss history
	- dis_loss_hist: dis loss history
	- dis_acc_hist: acuracy of dis at correctly classifying real images (from validation data)
	"""
	# Create batch of latent vectors to visualize
	# the progression of the generator
	fixed_latent_vectors = torch.randn((64, nz), device=device)
	gen_test_imgs = []
	gen_loss_hist = []
	dis_loss_hist = []
	dis_acc_hist = []

	gen = gen.to(device=device)
	dis = dis.to(device=device)
	gen.train()  # put the networks in training mode
	dis.train()

	tic = time.perf_counter()  # start timer for model saves
	timed_save = False
	print("\nStarting Training of DCGAN")
	for e in range(num_epochs):
		for t, (x, y) in enumerate(loader_train):
			x = x.to(device=device, dtype=DTYPE)
			num_ims = x.size()[0]  # this should equal batch size except for last batch in epoch

			# Perform Dis training update ( minimize -ylog(D(x)) - (1-y)log(1-D(G(z)) )
			# 1) Classify a real data batch using Dis
			dis.zero_grad()

			# create labels for real images  w/ one-sided label smoothing
			labels = smooth_labels(t, e, num_ims)
			logits = dis(x)  # real_scores
			loss_dis_real = loss_fn(logits, labels)
			loss_dis_real.backward()  # compute gradients

			# 2) Classify a fake data batch (from Gen) using Dis
			z = torch.randn((num_ims, nz), device=device)
			fake_imgs = gen(z)
			labels.fill_(0.0)  # create labels for fake images (no smoothing)
			logits = dis(fake_imgs.detach())  # detach() stops gradients being propagated through gen()
			loss_dis_fake = loss_fn(logits, labels)
			loss_dis_fake.backward()

			# 3) Adam update on Dis
			dis_loss_hist.append(loss_dis_real.item() + loss_dis_fake.item())
			optimizer_d.step()

			# Perform Gen training update ( minimize -log(D(G(z)) )
			gen.zero_grad()
			labels.fill_(1.0)  # label swap trick
			logits = dis(fake_imgs)
			loss_gen = loss_fn(logits, labels)
			loss_gen.backward()
			gen_loss_hist.append(loss_gen.item())
			optimizer_g.step()

			if t % PRINT_EVERY == 0:
				# Calculate performance stats and display them
				dis_acc_real = evaluate_accuracy(dis, loader_val)
				dis_acc_hist.append(dis_acc_real)
				print(f"Epoch: {e}/{num_epochs}\t Iteration: {t}\nDis loss: {dis_loss_hist[-1]:.3f} | "
						f"Gen loss: {gen_loss_hist[-1]:.3f} | Dis accuracy (real images): {dis_acc_hist[-1]:.3f}")

			if (t + 1) % TEST_GEN_EVERY == 0 or ((e == num_epochs - 1) and t == len(loader_train) - 1):
				# Create images using gen to visualize progress
				with torch.no_grad():
					ims = gen(fixed_latent_vectors).detach()
				img_array = vutils.make_grid(ims, padding=2, normalize=True)
				gen_test_imgs.append(img_array)
				img_name = IMAGE_SAVE_PATH + "/GAN_Gen_" + str(t) + ".jpeg"
				save_image(img_array, img_name)
				# *** uncomment to see images produced by gen during training (unlikely to work on remote server) ***
				# plt.imshow(np.transpose(gen_test_imgs[-1].cpu(), (1, 2, 0)))
				# plt.show()

			# Check if time to save model
			toc = time.perf_counter()
			time_diff = toc - tic
			if time_diff > SAVE_TIME_INTERVAL:
				tic = time.perf_counter()  # reset clock
				timed_save = True
				print("Timed save")

			if (SAVE_EVERY and (t + 1) % SAVE_EVERY == 0) or timed_save:
				# Save the model weights in a folder labelled with the validation accuracy
				save_model_params(gen, "gen", SAVE_PATH, e, optimizer_g, gen_loss_hist[-1])
				save_model_params(dis, "dis", SAVE_PATH, e, optimizer_d, dis_loss_hist[-1])
				timed_save = False

	return gen_test_imgs, gen_loss_hist, dis_loss_hist, dis_acc_hist


def smooth_labels(iteration, epoch, num_labels):
	"""
	Creates labels using one-sided label smoothing
	"""
	if iteration > label_smoothing_start or epoch > 0:
		smoothing = 0.9  # replace the true label (1.0) with a less exact label
	else:
		smoothing = 1.0  # use true label for first few iterations
	return torch.full((num_labels, 1), smoothing, device=device, dtype=DTYPE)


def evaluate_accuracy(model, data_loader, num_eval=500):
	"""
	Evaluates accuracy of a model on a dataset
	Assumes the loss is based on a sigmoid and that
	the model outputs logits (un-normalized log probabilities)
	"""
	num_correct = 0
	num_samples = 0
	model = model.to(device=device)
	model.eval()
	count = 0
	with torch.no_grad():
		for t, (x, y) in enumerate(data_loader):
			x = x.to(device=device, dtype=DTYPE)
			y = y.to(device=device, dtype=DTYPE)
			num_samples += x.size()[0]

			scores = model(x)
			predictions = (torch.sigmoid(scores) > 0.5).float()
			num_correct += float((predictions == y.view(y.size()[0],1)).sum())
			count += x.size()[0]
			if count > num_eval:  # Test for specified num of data points
				break
	return num_correct / num_samples


def load_feature_extraction_data(root_path):
	"""
	Loads the 3 dataset splits for automatically labelled positive data
	from GONet dataset

	Inputs:
	- root_path: absolute path to the root folder of the dataset

	Returns:
	- data_loaders: dictionary of pytorch Dataset objects
		{"train":, "test":, "val":, "train_labelled", "test_labelled", "val_labelled"}
	"""
	transform = T.Compose([
		T.RandomHorizontalFlip(p=0.5),  # Flip image horizontally with p % chance
		T.ToTensor(),
		Normalize()])  # Convert images to range [-1, 1]

	# Create data_set objects for each of the data splits
	# Positive automatically labelled data
	train_pos = DataSetDCGAN(root_path, "train", transform=transform)
	val_pos = DataSetDCGAN(root_path, "vali", transform=transform)
	test_pos = DataSetDCGAN(root_path, "test", transform=transform)
	print("Loaded Automatically Labelled, Positive Datasets")
	data_sets = [train_pos, val_pos, test_pos]
	display_num_images(data_sets)

	# Create DataLoaders for the data splits
	loader_train = DataLoader(train_pos, batch_size=batch_size, drop_last=True,
							  sampler=RandomSampler(train_pos), num_workers=WORKERS)
	loader_val = DataLoader(val_pos, batch_size=batch_size, drop_last=True,
							sampler=RandomSampler(val_pos), num_workers=WORKERS)
	loader_test = DataLoader(test_pos, batch_size=batch_size, drop_last=True,
							 sampler=RandomSampler(test_pos), num_workers=WORKERS)
	data_loaders = {"train": loader_train, "val": loader_val, "test": loader_test}
	return data_loaders, data_sets


def main():
	"""Run training of DCGAN"""
	data_loaders, data_sets = load_feature_extraction_data(DATA_PATH)

	# plot some training examples
	plot_examples = False
	if plot_examples:
		real_batch = next(iter(data_loaders["train"]))
		plt.figure(figsize=(8, 8))
		plt.axis("off")
		plt.title("Training Images")
		plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:batch_size], padding=2,
		                                         normalize=True).cpu(), (1, 2, 0)))
		plt.show()

	# Create the Generator and Discriminator networks
	gen = Generator()
	gen.apply(weights_init)
	dis = Discriminator()
	dis.apply(weights_init)

	# Set up for training
	optimizer_g = optim.Adam(gen.parameters(), lr=lr_gen, betas=(beta1, 0.999))
	optimizer_d = optim.Adam(dis.parameters(), lr=lr_dis, betas=(beta1, 0.999))

	# Combines a sigmoid (converts logits to probabilites) with Binary cross-entropy loss
	loss = nn.BCEWithLogitsLoss()

	# Train the DCGAN network
	gen_test_imgs, gen_loss_hist, dis_loss_hist, dis_acc_hist = \
		train_dcgan(gen, dis, optimizer_g, optimizer_d, loss, data_loaders["train"], data_loaders["val"])

	# Plot loss
	plt.figure(figsize=(10,5))
	plt.title("Generator and Discriminator Training Loss")
	plt.plot(gen_loss_hist, label="gen")
	plt.plot(dis_loss_hist, label="dis")
	plt.xlabel("iteration")
	plt.ylabel("loss")
	plt.legend()
	plt.show()
	plt.savefig("loss.png")

	# Plot some real images and fake images
	# real_batch = next(iter(data_loaders["train"]))
	# plt.figure(figsize=(15, 15))
	# plt.subplot(1, 2, 1)
	# plt.axis("off")
	# plt.title("Real Images")
	# # plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:batch_size], padding=2,
	                                        #  normalize=True).cpu(), (1, 2, 0)))
	# Plot the fake images from the last epoch
	# plt.subplot(1, 2, 2)
	# plt.axis("off")
	# plt.title("Fake Images")
	# plt.imshow(np.transpose(gen_test_imgs[-1], (1, 2, 0)))
	# plt.show()

if __name__ == "__main__":
	main()
